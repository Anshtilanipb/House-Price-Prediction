{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpdezhXPsx4J"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "\n",
        "\n",
        "\n",
        "# Get clean data\n",
        "path = r\"https://drive.google.com/uc?export=download&id=1P49POlAk27uRzWKXoR2WaEfb1lyyfiRJ\" # oh_encoded_data.csv from drive\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv(path)\n",
        "df.shape\n",
        "\n",
        "df.shape\n",
        "\n",
        "df.head()\n",
        "\n",
        "df = df.drop(['Unnamed: 0'], axis=1)\n",
        "df.head()\n",
        "\n",
        "df.shape\n",
        "\n",
        "\"\"\"## Split Dataset in train and test\"\"\"\n",
        "\n",
        "X = df.drop(\"price\", axis=1)\n",
        "y = df['price']\n",
        "print('Shape of X = ', X.shape)\n",
        "print('Shape of y = ', y.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 51)\n",
        "print('Shape of X_train = ', X_train.shape)\n",
        "print('Shape of y_train = ', y_train.shape)\n",
        "print('Shape of X_test = ', X_test.shape)\n",
        "print('Shape of y_test = ', y_test.shape)\n",
        "\n",
        "\"\"\"## Feature Scaling\"\"\"\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train= sc.transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\"\"\"## Machine Learning Model Training\n",
        "\n",
        "## Linear Regression\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "lr = LinearRegression()\n",
        "lr_lasso = Lasso()\n",
        "lr_ridge = Ridge()\n",
        "\n",
        "def rmse(y_test, y_pred):\n",
        "  return np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "lr_score = lr.score(X_test, y_test)\n",
        "lr_rmse = rmse(y_test, lr.predict(X_test))\n",
        "lr_score, lr_rmse\n",
        "\n",
        "# Lasso\n",
        "lr_lasso.fit(X_train, y_train)\n",
        "lr_lasso_score=lr_lasso.score(X_test, y_test)\n",
        "lr_lasso_rmse = rmse(y_test, lr_lasso.predict(X_test))\n",
        "lr_lasso_score, lr_lasso_rmse\n",
        "\n",
        "\"\"\"## Support Vector Machine\"\"\"\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "svr = SVR()\n",
        "svr.fit(X_train,y_train)\n",
        "svr_score=svr.score(X_test,y_test)\n",
        "svr_rmse = rmse(y_test, svr.predict(X_test))\n",
        "svr_score, svr_rmse\n",
        "\n",
        "\"\"\"## Random Forest Regressor\"\"\"\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rfr = RandomForestRegressor()\n",
        "rfr.fit(X_train,y_train)\n",
        "rfr_score=rfr.score(X_test,y_test)\n",
        "rfr_rmse = rmse(y_test, rfr.predict(X_test))\n",
        "rfr_score, rfr_rmse\n",
        "\n",
        "\"\"\"## XGBoost\"\"\"\n",
        "\n",
        "import xgboost\n",
        "xgb_reg = xgboost.XGBRegressor()\n",
        "xgb_reg.fit(X_train,y_train)\n",
        "xgb_reg_score=xgb_reg.score(X_test,y_test)\n",
        "xgb_reg_rmse = rmse(y_test, xgb_reg.predict(X_test))\n",
        "xgb_reg_score, xgb_reg_rmse\n",
        "\n",
        "print(pd.DataFrame([{'Model': 'Linear Regression','Score':lr_score, \"RMSE\":lr_rmse},\n",
        "              {'Model': 'Lasso','Score':lr_lasso_score, \"RMSE\":lr_lasso_rmse},\n",
        "              {'Model': 'Support Vector Machine','Score':svr_score, \"RMSE\":svr_rmse},\n",
        "              {'Model': 'Random Forest','Score':rfr_score, \"RMSE\":rfr_rmse},\n",
        "              {'Model': 'XGBoost','Score':xgb_reg_score, \"RMSE\":xgb_reg_rmse}],\n",
        "             columns=['Model','Score','RMSE']))\n",
        "\n",
        "\"\"\"## Cross Validation\"\"\"\n",
        "\n",
        "'''from sklearn.model_selection import KFold,cross_val_score\n",
        "cvs = cross_val_score(xgb_reg, X_train,y_train, cv = 10)\n",
        "cvs, cvs.mean() '''\n",
        "\n",
        "'''cvs_rfr = cross_val_score(rfr, X_train,y_train, cv = 10)\n",
        "cvs_rfr, cvs_rfr.mean() '''\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cvs_rfr2 = cross_val_score(RandomForestRegressor(), X_train,y_train, cv = 10)\n",
        "cvs_rfr2, cvs_rfr2.mean()\n",
        "\n",
        "\"\"\"# Hyper Parmeter Tuning\"\"\"\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost.sklearn import XGBRegressor\n",
        "'''\n",
        "# Various hyper-parameters to tune\n",
        "xgb1 = XGBRegressor()\n",
        "parameters = {'learning_rate': [0.1,0.03, 0.05, 0.07], #so called `eta` value, # [default=0.3] Analogous to learning rate in GBM\n",
        "              'min_child_weight': [1,3,5], #[default=1] Defines the minimum sum of weights of all observations required in a child.\n",
        "              'max_depth': [4, 6, 8], #[default=6] The maximum depth of a tree,\n",
        "              'gamma':[0,0.1,0.001,0.2], #Gamma specifies the minimum loss reduction required to make a split.\n",
        "              'subsample': [0.7,1,1.5], #Denotes the fraction of observations to be randomly samples for each tree.\n",
        "              'colsample_bytree': [0.7,1,1.5], #Denotes the fraction of columns to be randomly samples for each tree.\n",
        "              'objective':['reg:linear'], #This defines the loss function to be minimized.\n",
        "\n",
        "              'n_estimators': [100,300,500]}\n",
        "\n",
        "xgb_grid = GridSearchCV(xgb1,\n",
        "                        parameters,\n",
        "                        cv = 2,\n",
        "                        n_jobs = -1,\n",
        "                        verbose=True)\n",
        "\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "\n",
        "print(xgb_grid.best_score_) # 0.9397345161940295\n",
        "print(xgb_grid.best_params_)'''\n",
        "\n",
        "'''xgb_tune = xgb_grid.estimator\n",
        "\n",
        "xgb_tune.fit(X_train,y_train) # 0.9117591385438816\n",
        "xgb_tune.score(X_test,y_test)'''\n",
        "\n",
        "'''cvs = cross_val_score(xgb_tune, X_train,y_train, cv = 10)\n",
        "cvs, cvs.mean() #  0.9645582338461773)'''\n",
        "\n",
        "#[i/10.0 for i in range(1,6)]\n",
        "\n",
        "#xgb_grid.estimator\n",
        "\n",
        "xgb_tune2 =  XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "             colsample_bynode=0.6, colsample_bytree=1, gamma=0,\n",
        "             importance_type='gain', learning_rate=0.25, max_delta_step=0,\n",
        "             max_depth=4, min_child_weight=1, missing=None, n_estimators=400,\n",
        "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
        "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "             silent=None, subsample=1, verbosity=1)\n",
        "xgb_tune2.fit(X_train,y_train)\n",
        "xgb_tune2.score(X_test,y_test)\n",
        "\n",
        "'''parameters = {'learning_rate': [0.1,0.03, 0.05, 0.07], #so called `eta` value, # [default=0.3] Analogous to learning rate in GBM\n",
        "              'min_child_weight': [1,3,5], #[default=1] Defines the minimum sum of weights of all observations required in a child.\n",
        "              'max_depth': [4, 6, 8], #[default=6] The maximum depth of a tree,\n",
        "              'gamma':[0,0.1,0.001,0.2], #Gamma specifies the minimum loss reduction required to make a split.\n",
        "              'subsample': [0.7,1,1.5], #Denotes the fraction of observations to be randomly samples for each tree.\n",
        "              'colsample_bytree': [0.7,1,1.5], #Denotes the fraction of columns to be randomly samples for each tree.\n",
        "              'objective':['reg:linear'], #This defines the loss function to be minimized.\n",
        "              'n_estimators': [100,300,500]}'''\n",
        "\n",
        "xgb_tune2 =  XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "             colsample_bynode=0.9, colsample_bytree=1, gamma=0,\n",
        "             importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
        "             max_depth=4, min_child_weight=5, missing=None, n_estimators=100,\n",
        "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
        "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "             silent=None, subsample=1, verbosity=1)\n",
        "xgb_tune2.fit(X_train,y_train) # 0.9412851220926807\n",
        "xgb_tune2.score(X_test,y_test)\n",
        "\n",
        "cvs = cross_val_score(xgb_tune2, X_train,y_train, cv = 5)\n",
        "cvs, cvs.mean() #  0.9706000326331659'''\n",
        "\n",
        "np.sqrt(mean_squared_error(y_test, xgb_tune2.predict(X_test)))\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"## Test Model\"\"\"\n",
        "\n",
        "list(X.columns)\n",
        "\n",
        "# it help to get predicted value of hosue  by providing features value\n",
        "def predict_house_price(model,bath,balcony,total_sqft_int,bhk,price_per_sqft,area_type,availability,location):\n",
        "\n",
        "  x =np.zeros(len(X.columns)) # create zero numpy array, len = 107 as input value for model\n",
        "\n",
        "  # adding feature's value accorind to their column index\n",
        "  x[0]=bath\n",
        "  x[1]=balcony\n",
        "  x[2]=total_sqft_int\n",
        "  x[3]=bhk\n",
        "  x[4]=price_per_sqft\n",
        "\n",
        "  if \"availability\"==\"Ready To Move\":\n",
        "    x[8]=1\n",
        "\n",
        "  if 'area_type'+area_type in X.columns:\n",
        "    area_type_index = np.where(X.columns==\"area_type\"+area_type)[0][0]\n",
        "    x[area_type_index] =1\n",
        "\n",
        "    #print(area_type_index)\n",
        "\n",
        "  if 'location_'+location in X.columns:\n",
        "    loc_index = np.where(X.columns==\"location_\"+location)[0][0]\n",
        "    x[loc_index] =1\n",
        "\n",
        "    #print(loc_index)\n",
        "\n",
        "\n",
        "  # feature scaling\n",
        "  x = sc.transform([x])[0] # give 2d np array for feature scaling and get 1d scaled np array\n",
        "\n",
        "\n",
        "  return model.predict([x])[0] # return the predicted value by train XGBoost model\n",
        "\n",
        "predict_house_price(model=xgb_tune2, bath=3,balcony=2,total_sqft_int=1672,bhk=3,price_per_sqft=8971.291866,area_type=\"Plot  Area\",availability=\"Ready To Move\",location=\"Devarabeesana Halli\")\n",
        "\n",
        "##test sample\n",
        "\n",
        "\n",
        "predict_house_price(model=xgb_tune2, bath=3,balcony=2,total_sqft_int=1750,bhk=3,price_per_sqft=8571.428571,area_type=\"Super built-up\",availability=\"Ready To Move\",location=\"Devarabeesana Halli\")\n",
        "\n",
        "##test sample\n",
        "\n",
        "\n",
        "predict_house_price(model=xgb_tune2,bath=3,balcony=3,total_sqft_int=1750,bhk=3,price_per_sqft=8514.285714,area_type=\"Built-up Area\",availability=\"Ready To Move\",location=\"Devarabeesana Halli\")\n",
        "\n",
        "\"\"\"# Save model & load model\"\"\"\n",
        "\n",
        "import joblib\n",
        "# save model\n",
        "joblib.dump(xgb_tune2, 'bangalore_house_price_prediction_model.pkl')\n",
        "joblib.dump(rfr, 'bangalore_house_price_prediction_rfr_model.pkl')\n",
        "\n",
        "# load model\n",
        "bangalore_house_price_prediction_model = joblib.load(\"bangalore_house_price_prediction_model.pkl\")\n",
        "\n",
        "# predict house price\n",
        "predict_house_price(bangalore_house_price_prediction_model,bath=3,balcony=3,total_sqft_int=150,bhk=3,price_per_sqft=8514.285714,area_type=\"Built-up Area\",availability=\"Ready To Move\",location=\"Devarabeesana Halli\")\n"
      ]
    }
  ]
}